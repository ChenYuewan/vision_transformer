{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vision Transformer AugReg",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXhZm0kpPpH6"
      },
      "source": [
        "##### Copyright 2021 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "KfmzfvFxPuk7"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOVCm4CnP1Do"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/google-research/vision_transformer/blob/master/vit_jax_augreg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dkWVp5qbziu"
      },
      "source": [
        "## How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofwi7yvlx5hj"
      },
      "source": [
        "Model repository published with the paper\n",
        "\n",
        "[**How to train your ViT? Data, Augmentation, and Regularization in Vision\n",
        "Transformers**](https://arxiv.org/abs/2106.10270)\n",
        "\n",
        "This Colab shows how to\n",
        "[find checkpoints](#scrollTo=F4SLGDtFxlsC)\n",
        "in the repository, how to\n",
        "[select and load a model](#scrollTo=wh_SLkQtQ6K4)\n",
        "form the repository and use it for inference\n",
        "([also with PyTorch](#scrollTo=1nMyWmDycpAo)),\n",
        "and how to\n",
        "[fine-tune on a dataset](#scrollTo=iAruT3YOxqB6).\n",
        "\n",
        "For more details, please refer to the repository:\n",
        "\n",
        "https://github.com/google-research/vision_transformer/\n",
        "\n",
        "Note that this Colab directly uses the unmodified code from the repository. If\n",
        "you want to modify the modules and persist your changes, you can do all that\n",
        "using free GPUs and TPUs without leaving the Colab environment - see\n",
        "\n",
        "https://colab.research.google.com/github/google-research/vision_transformer/blob/master/vit_jax.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPNkX_dfCNog"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ihyJdwbCVmW",
        "outputId": "c59b1541-b99e-43ac-9dd5-ecc799391acc"
      },
      "source": [
        "# Fetch vision_transformer repository.\n",
        "![ -d vision_transformer ] || git clone --depth=1 https://github.com/google-research/vision_transformer\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'vision_transformer'...\n",
            "remote: Enumerating objects: 49, done.\u001b[K\n",
            "remote: Counting objects: 100% (49/49), done.\u001b[K\n",
            "remote: Compressing objects: 100% (40/40), done.\u001b[K\n",
            "remote: Total 49 (delta 6), reused 29 (delta 5), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (49/49), 2.02 MiB | 17.95 MiB/s, done.\n",
            "Resolving deltas: 100% (6/6), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__7nV7c6C4Zn",
        "outputId": "2d760997-9c4e-4cb3-b348-9fef179f9347"
      },
      "source": [
        "# Install dependencies.\n",
        "!pip install -qr vision_transformer/vit_jax/requirements.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33mWARNING: jax 0.5.3 does not provide the extra 'cuda11-cudnn86'\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: jax 0.5.3 does not provide the extra 'cuda11-cudnn86'\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: jax 0.5.3 does not provide the extra 'cuda11-cudnn86'\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m901.6/901.6 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.7/76.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.9/251.9 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for flaxformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9VrgMtUC2w-",
        "outputId": "ede97899-9037-4bb7-dd56-d4f3968c48c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "# Import files from repository.\n",
        "\n",
        "import sys\n",
        "if './vision_transformer' not in sys.path:\n",
        "  sys.path.append('./vision_transformer')\n",
        "\n",
        "#%load_ext autoreload\n",
        "#%autoreload 2\n",
        "\n",
        "from vit_jax import checkpoint\n",
        "from vit_jax import models\n",
        "from vit_jax import train\n",
        "from vit_jax.configs import augreg as augreg_config\n",
        "from vit_jax.configs import models as models_config"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotFoundError",
          "evalue": "/usr/local/lib/python3.12/dist-packages/tensorflow_text/python/ops/_boise_offset_converter.so: undefined symbol: _ZN6tflite4shim23TfShapeInferenceContextC1EPN10tensorflow15shape_inference16InferenceContextE",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-348097062.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvit_jax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mvit_jax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvit_jax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvit_jax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfigs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maugreg\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0maugreg_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/vision_transformer/vit_jax/models.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# limitations under the License.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mvit_jax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels_lit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvit_jax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels_mixer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvit_jax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels_vit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/vision_transformer/vit_jax/models_lit.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvit_jax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvit_jax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels_vit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mvit_jax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mflaxformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marchitectures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbert\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/vision_transformer/vit_jax/preprocess.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow_text/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# pylint: disable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpybinds\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtflite_registrar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow_text/python/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# pylint: disable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Public symbols in the \"tensorflow_text.layers\" package.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow_text/python/keras/layers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# pylint: disable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_layers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Public symbols in the \"tensorflow_text.layers\" package.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow_text/python/keras/layers/tokenization_layers.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlookup_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mragged\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mragged_conversion_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0municode_script_tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwhitespace_tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwordpiece_tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow_text/python/ops/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpybinds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpywrap_fast_wordpiece_tokenizer_model_builder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuild_fast_wordpiece_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert_tokenizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboise_offset_converter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mboise_tags_to_offsets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboise_offset_converter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moffsets_to_boise_tags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyte_splitter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mByteSplitter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow_text/python/ops/boise_offset_converter.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_library\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplatform\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mresource_loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mgen_boise_offset_converter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_library\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_op_library\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_path_to_datafile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_boise_offset_converter.so'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/load_library.py\u001b[0m in \u001b[0;36mload_op_library\u001b[0;34m(library_filename)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mRuntimeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0munable\u001b[0m \u001b[0mto\u001b[0m \u001b[0mload\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlibrary\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mget\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpython\u001b[0m \u001b[0mwrappers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m   \"\"\"\n\u001b[0;32m---> 54\u001b[0;31m   \u001b[0mlib_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpy_tf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_LoadLibrary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibrary_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     wrappers = _pywrap_python_op_gen.GetPythonWrappers(\n",
            "\u001b[0;31mNotFoundError\u001b[0m: /usr/local/lib/python3.12/dist-packages/tensorflow_text/python/ops/_boise_offset_converter.so: undefined symbol: _ZN6tflite4shim23TfShapeInferenceContextC1EPN10tensorflow15shape_inference16InferenceContextE"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpYDX1idbVxq"
      },
      "source": [
        "# Connect to TPUs if runtime type is of type TPU.\n",
        "\n",
        "import os\n",
        "if 'google.colab' in str(get_ipython()) and 'COLAB_TPU_ADDR' in os.environ:\n",
        "  import jax\n",
        "  import jax.tools.colab_tpu\n",
        "  jax.tools.colab_tpu.setup_tpu()\n",
        "  print('Connected to TPU.')\n",
        "else:\n",
        "  # Otherwise print information about GPU.\n",
        "  !nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xTeXue8DfwL"
      },
      "source": [
        "# Some more imports used in this Colab.\n",
        "\n",
        "import glob\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import time\n",
        "\n",
        "from absl import logging\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "pd.options.display.max_colwidth = None\n",
        "logging.set_verbosity(logging.INFO)  # Shows logs during training."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4SLGDtFxlsC"
      },
      "source": [
        "### Explore checkpoints\n",
        "\n",
        "This section contains shows how to use the `index.csv` table for model\n",
        "selection.\n",
        "\n",
        "See\n",
        "[`vit_jax.checkpoint.get_augreg_df()`](https://github.com/google-research/vision_transformer/blob/ed1491238f5ff6099cca81087c575a215281ed14/vit_jax/checkpoint.py#L181-L228)\n",
        "for a detailed description of the individual columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yy-cuGxyD6Xw"
      },
      "source": [
        "# Load master table from Cloud.\n",
        "with tf.io.gfile.GFile('gs://vit_models/augreg/index.csv') as f:\n",
        "  df = pd.read_csv(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8iAESfDKCQS"
      },
      "source": [
        "# This is a pretty large table with lots of columns:\n",
        "print(f'loaded {len(df):,} rows')\n",
        "df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wi4CwizkKIx_"
      },
      "source": [
        "# Number of distinct checkpoints\n",
        "len(tf.io.gfile.glob('gs://vit_models/augreg/*.npz'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOM3HkkbKPdk"
      },
      "source": [
        "# Any column prefixed with \"adapt_\" pertains to the fine-tuned checkpoints.\n",
        "# Any column without that prefix pertains to the pre-trained checkpoints.\n",
        "len(set(df.filename)), len(set(df.adapt_filename))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3EyLd3S4XDS"
      },
      "source": [
        "df.name.unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w38Gjj-SGLVu"
      },
      "source": [
        "# Upstream AugReg parameters (section 3.3):\n",
        "(\n",
        "df.groupby(['ds', 'name', 'wd', 'do', 'sd', 'aug']).filename\n",
        "  .count().unstack().unstack().unstack()\n",
        "  .dropna(1, 'all').fillna(0).astype(int)\n",
        "  .iloc[:7]  # Just show beginning of a long table.\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjI5jX36GeFr"
      },
      "source": [
        "# Downstream parameters (table 4)\n",
        "# (Imbalance in 224 vs. 384 is due to recently added B/8 checkpoints)\n",
        "(\n",
        "df.groupby(['adapt_resolution', 'adapt_ds', 'adapt_lr', 'adapt_steps']).filename\n",
        "  .count().astype(str).unstack().unstack()\n",
        "  .dropna(1, 'all').fillna('')\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Gbt7cdhEHg7"
      },
      "source": [
        "# Let's first select the \"best checkpoint\" for every model. We show in the\n",
        "# paper (section 4.5) that one can get a good performance by simply choosing the\n",
        "# best model by final pre-train validation accuracy (\"final-val\" column).\n",
        "# Pre-training with imagenet21k 300 epochs (ds==\"i21k\") gives the best\n",
        "# performance in almost all cases (figure 6, table 5).\n",
        "best_filenames = set(\n",
        "    df.query('ds==\"i21k\"')\n",
        "    .groupby('name')\n",
        "    .apply(lambda df: df.sort_values('final_val').iloc[-1])\n",
        "    .filename\n",
        ")\n",
        "\n",
        "# Select all finetunes from these models.\n",
        "best_df = df.loc[df.filename.apply(lambda filename: filename in best_filenames)]\n",
        "\n",
        "# Note: 9 * 68 == 612\n",
        "len(best_filenames), len(best_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poiez-EcZmtI"
      },
      "source": [
        "best_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI_XTt1vX930"
      },
      "source": [
        "# Note that this dataframe contains the models from the \"i21k_300\" column of\n",
        "# table 3:\n",
        "best_df.query('adapt_ds==\"imagenet2012\"').groupby('name').apply(\n",
        "    lambda df: df.sort_values('adapt_final_val').iloc[-1]\n",
        ")[[\n",
        "   # Columns from upstream\n",
        "   'name', 'ds', 'filename',\n",
        "   # Columns from downstream\n",
        "   'adapt_resolution', 'infer_samples_per_sec','adapt_ds', 'adapt_final_test', 'adapt_filename',\n",
        "]].sort_values('infer_samples_per_sec')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPYUh4V1EdHg"
      },
      "source": [
        "# Visualize the 2 (resolution) * 9 (models) * 8 (lr, steps) finetunings for a\n",
        "# single dataset (Pets37).\n",
        "# Note how larger models get better scores up to B/16 @384 even on this tiny\n",
        "# dataset, if pre-trained sufficiently.\n",
        "sns.relplot(\n",
        "    data=best_df.query('adapt_ds==\"oxford_iiit_pet\"'),\n",
        "    x='infer_samples_per_sec',\n",
        "    y='adapt_final_val',\n",
        "    hue='name',\n",
        "    style='adapt_resolution'\n",
        ")\n",
        "plt.gca().set_xscale('log');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9i7Y9AcpFtlr"
      },
      "source": [
        "# More details for a single pre-trained checkpoint.\n",
        "best_df.query('name==\"R26+S/32\" and adapt_ds==\"oxford_iiit_pet\"')[[\n",
        "  col for col in best_df.columns if col.startswith('adapt_')\n",
        "]].sort_values('adapt_final_val')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wh_SLkQtQ6K4"
      },
      "source": [
        "### Load a checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwWBMJSpQ7EZ"
      },
      "source": [
        "# Select a value from \"adapt_filename\" above that is a fine-tuned checkpoint.\n",
        "filename = 'R26_S_32-i21k-300ep-lr_0.001-aug_light1-wd_0.1-do_0.0-sd_0.0--oxford_iiit_pet-steps_0k-lr_0.003-res_384'\n",
        "\n",
        "tfds_name = filename.split('--')[1].split('-')[0]\n",
        "model_config = models_config.AUGREG_CONFIGS[filename.split('-')[0]]\n",
        "resolution = int(filename.split('_')[-1])\n",
        "path = f'gs://vit_models/augreg/{filename}.npz'\n",
        "\n",
        "print(f'{tf.io.gfile.stat(path).length / 1024 / 1024:.1f} MiB - {path}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKW2flB_Sk1u"
      },
      "source": [
        "# Fetch dataset that the checkpoint was finetuned on.\n",
        "# (Note that automatic download does not work with imagenet2012)\n",
        "ds, ds_info = tfds.load(tfds_name, with_info=True)\n",
        "ds_info"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtmV4tRCSX8n"
      },
      "source": [
        "# Get model instance - no weights are initialized yet.\n",
        "model = models.VisionTransformer(\n",
        "    num_classes=ds_info.features['label'].num_classes, **model_config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GU6Tw1KvScBA"
      },
      "source": [
        "# Load a checkpoint from cloud - for large checkpoints this can take a while...\n",
        "params = checkpoint.load(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Js-QtAqeUJCo"
      },
      "source": [
        "# Get a single example from dataset for inference.\n",
        "d = next(iter(ds['test']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpovSMYFUHAj"
      },
      "source": [
        "def pp(img, sz):\n",
        "  \"\"\"Simple image preprocessing.\"\"\"\n",
        "  img = tf.cast(img, float) / 255.0\n",
        "  img = tf.image.resize(img, [sz, sz])\n",
        "  return img\n",
        "\n",
        "plt.imshow(pp(d['image'], resolution));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBgTdceOT9vz"
      },
      "source": [
        "# Inference on batch with single example.\n",
        "logits, = model.apply({'params': params}, pp(d['image'], resolution).numpy()[None], train=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIQM--g4VwY9"
      },
      "source": [
        "# Plot logits (you can use tf.nn.softmax() to show probabilities instead).\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.bar(list(map(ds_info.features['label'].int2str, range(len(logits)))), logits)\n",
        "plt.xticks(rotation=90);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nMyWmDycpAo"
      },
      "source": [
        "#### Using `timm`\n",
        "\n",
        "If you know PyTorch, you're probably already familiar with `timm`.\n",
        "\n",
        "If not yet - it's your lucky day! Please check out their docs here:\n",
        "\n",
        "https://rwightman.github.io/pytorch-image-models/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yV6Sr1TdXUMp"
      },
      "source": [
        "# Checkpoints can also be loaded directly into timm...\n",
        "!pip install timm\n",
        "import timm\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwV4IvfYXioI"
      },
      "source": [
        "# For available model names, see here:\n",
        "# https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n",
        "# https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer_hybrid.py\n",
        "timm_model = timm.create_model(\n",
        "    'vit_small_r26_s32_384', num_classes=ds_info.features['label'].num_classes)\n",
        "\n",
        "# Non-default checkpoints need to be loaded from local files.\n",
        "if not tf.io.gfile.exists(f'{filename}.npz'):\n",
        "  tf.io.gfile.copy(f'gs://vit_models/augreg/{filename}.npz', f'{filename}.npz')\n",
        "timm.models.load_checkpoint(timm_model, f'{filename}.npz')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gd6NVWNLYOSI"
      },
      "source": [
        "def pp_torch(img, sz):\n",
        "  \"\"\"Simple image preprocessing for PyTorch.\"\"\"\n",
        "  img = pp(img, sz)\n",
        "  img = img.numpy().transpose([2, 0, 1])  # PyTorch expects NCHW format.\n",
        "  return torch.tensor(img[None])\n",
        "\n",
        "with torch.no_grad():\n",
        "  logits, = timm_model(pp_torch(d['image'], resolution)).detach().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vA-mEJofeHxd"
      },
      "source": [
        "# Same results as above (since we loaded the same checkpoint).\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.bar(list(map(ds_info.features['label'].int2str, range(len(logits)))), logits)\n",
        "plt.xticks(rotation=90);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAruT3YOxqB6"
      },
      "source": [
        "### Fine-tune\n",
        "\n",
        "You want to be connected to a TPU or GPU runtime for fine-tuning.\n",
        "\n",
        "Note that here we're just calling into the code. For more details see the\n",
        "annotated Colab\n",
        "\n",
        "https://colab.research.google.com/github/google-research/vision_transformer/blob/linen/vit_jax.ipynb\n",
        "\n",
        "Also note that Colab GPUs and TPUs are not very powerful. To run this code on\n",
        "more powerful machines, see:\n",
        "\n",
        "https://github.com/google-research/vision_transformer/#running-on-cloud\n",
        "\n",
        "In particular, note that due to the Colab \"TPU Node\" setup, transfering data to\n",
        "the TPUs is realtively slow (for example the smallest `R+Ti/16` model trains\n",
        "faster on a single GPU than on 8 TPUs...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiHEFXlBbZiV"
      },
      "source": [
        "#### TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fu6uaCGDXLmr"
      },
      "source": [
        "# Launch tensorboard before training - maybe click \"reload\" during training.\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=./workdirs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYU9EdV2bWV7"
      },
      "source": [
        "#### From tfds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1Ee2-GzZ9sl"
      },
      "source": [
        "# Create a new temporary workdir.\n",
        "workdir = f'./workdirs/{int(time.time())}'\n",
        "workdir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SB73ZFGCxsXo"
      },
      "source": [
        "# Get config for specified model.\n",
        "\n",
        "# Note that we can specify simply the model name (in which case the recommended\n",
        "# checkpoint for that model is taken), or it can be specified by its full\n",
        "# name.\n",
        "config = augreg_config.get_config('R_Ti_16')\n",
        "\n",
        "# A very small tfds dataset that only has a \"train\" split. We use this single\n",
        "# split both for training & evaluation by splitting it further into 90%/10%.\n",
        "config.dataset = 'tf_flowers'\n",
        "config.pp.train = 'train[:90%]'\n",
        "config.pp.test = 'train[90%:]'\n",
        "# tf_flowers only has 3670 images - so the 10% evaluation split will contain\n",
        "# 360 images. We specify batch_eval=120 so we evaluate on all but 7 of those\n",
        "# images (remainder is dropped).\n",
        "config.batch_eval = 120\n",
        "\n",
        "# Some more parameters that you will often want to set manually.\n",
        "# For example for VTAB we used steps={500, 2500} and lr={.001, .003, .01, .03}\n",
        "config.base_lr = 0.01\n",
        "config.shuffle_buffer = 1000\n",
        "config.total_steps = 100\n",
        "config.warmup_steps = 10\n",
        "config.accum_steps = 0  # Not needed with R+Ti/16 model.\n",
        "config.pp['crop'] = 224"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldKZFMnmYjY_"
      },
      "source": [
        "# Call main training loop. See repository and above Colab for details.\n",
        "state = train.train_and_evaluate(config, workdir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOtCfWFOfVFq"
      },
      "source": [
        "#### From JPG files\n",
        "\n",
        "The codebase supports training directly form JPG files on the local filesystem\n",
        "instead of `tfds` datasets. Note that the throughput is somewhat reduced, but\n",
        "that only is noticeable for very small models.\n",
        "\n",
        "The main advantage of `tfds` datasets is that they are versioned and available\n",
        "globally."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VxlDlnxKAa3"
      },
      "source": [
        "base = '.'  # Store data on VM (ephemeral)."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yr6JY8OBmKkj"
      },
      "source": [
        "# Uncomment below lines if you want to download & persist files in your Google\n",
        "# Drive instead. Note that Colab VMs are reset (i.e. files are deleted) after\n",
        "# some time of inactivity. Storing data to Google Drive guarantees that it is\n",
        "# still available next time you connect from a new VM.\n",
        "\n",
        "# Note that this is significantly slower than reading from the VMs locally\n",
        "# attached file system!\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/gdrive')\n",
        "# base = '/gdrive/My Drive/vision_transformer_images'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZJdKYv3mLqd"
      },
      "source": [
        "# Download some dataset & unzip.\n",
        "! rm -rf $base/flower_photos; mkdir -p $base\n",
        "! (cd $base && curl https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz | tar xz)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QRnYHLzmPxL"
      },
      "source": [
        "# Since the default file format of above \"tf_flowers\" dataset is\n",
        "# flower_photos/{class_name}/{filename}.jpg\n",
        "# we first need to split it into a \"train\" (90%) and a \"test\" (10%) set:\n",
        "# flower_photos/train/{class_name}/{filename}.jpg\n",
        "# flower_photos/test/{class_name}/{filename}.jpg\n",
        "\n",
        "def split(base_dir, test_ratio=0.1):\n",
        "  paths = glob.glob(f'{base_dir}/*/*.jpg')\n",
        "  random.shuffle(paths)\n",
        "  counts = dict(test=0, train=0)\n",
        "  for i, path in enumerate(paths):\n",
        "    split = 'test' if i < test_ratio * len(paths) else 'train'\n",
        "    *_, class_name, basename = path.split('/')\n",
        "    dst = f'{base_dir}/{split}/{class_name}/{basename}'\n",
        "    if not os.path.isdir(os.path.dirname(dst)):\n",
        "      os.makedirs(os.path.dirname(dst))\n",
        "    shutil.move(path, dst)\n",
        "    counts[split] += 1\n",
        "  print(f'Moved {counts[\"train\"]:,} train and {counts[\"test\"]:,} test images.')\n",
        "\n",
        "split(f'{base}/flower_photos')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DKwlCXTKJjJ"
      },
      "source": [
        "# Create a new temporary workdir.\n",
        "workdir = f'./workdirs/{int(time.time())}'\n",
        "workdir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2ojstZeKJjK"
      },
      "source": [
        "# Read data from directory containing files.\n",
        "# (See cell above for more config settings)\n",
        "config.dataset = f'{base}/flower_photos'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8hljjZlKJjK"
      },
      "source": [
        "# And fine-tune on images provided\n",
        "opt = train.train_and_evaluate(config, workdir)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}